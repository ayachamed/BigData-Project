\documentclass[a4paper,12pt]{article}

% ==============================================================================
% PACKAGES & SETUP
% ==============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}      % Better font encoding
\usepackage{mathpazo}         % Palatino font (Pro, Clean, Serif)
\usepackage[scaled=0.95]{helvet} % Helvetica for Sans-Serif
\usepackage{courier}          % Courier for code
\usepackage{microtype}        % PRO TIP: Improves character spacing/kerning
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{listings} 
\usepackage{subcaption} 
\usepackage{svg}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{parskip}          % Clean paragraph spacing (no indentation)

% --- Page Geometry ---
\geometry{a4paper, margin=2.5cm}

% --- Brand Colors ---
% NOTE: xcolor color names are case-sensitive.
% Some templates/logs refer to `SECONDARYCOLOR`, so alias it to the defined color.
\definecolor{primaryColor}{HTML}{2B85BB} 
\definecolor{secondaryColor}{HTML}{0F0E0E}
\colorlet{SECONDARYCOLOR}{secondaryColor}
\definecolor{codeGreen}{rgb}{0,0.6,0}
\definecolor{codeGray}{rgb}{0.5,0.5,0.5}
\definecolor{codePurple}{rgb}{0.58,0,0.82}
\definecolor{lightGray}{HTML}{F8F9FA} % Slightly lighter for cleaner look

% --- Hyperlink Setup ---
\hypersetup{
    colorlinks=true,
    linkcolor=primaryColor,
    filecolor=secondaryColor,      
    urlcolor=primaryColor,
    pdftitle={Big Data Analysis Report},
    pdfpagemode=FullScreen,
}

% --- Code Listing Settings ---
\lstset{ 
    language=Python,               
    backgroundcolor=\color{lightGray},   
    commentstyle=\color{codeGreen}\itshape,
    keywordstyle=\color{primaryColor}\bfseries,
    numberstyle=\tiny\color{codeGray},
    stringstyle=\color{codePurple},
    basicstyle=\ttfamily\footnotesize, % Clean Monospace font
    breaklines=true,                 
    breakatwhitespace=false,         
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}, 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=8pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=lines, % Cleaner frame style (top and bottom lines only)
    rulecolor=\color{codeGray}
}

% --- Path to images ---
\graphicspath{{./}{../outputs/}{../docs/}}
\svgpath{{./}{../docs/}} 

% --- Header and Footer ---
\pagestyle{fancy}
\fancyhf{}
% Header
\fancyhead[L]{\textcolor{primaryColor}{\bfseries\normalfont Big Data Project: YouTube Analytics}}
\fancyhead[R]{\thepage}

% Footer
\fancyfoot[C]{\textcolor{secondaryColor}{\small\normalfont Hadoop \& Spark Analysis of Gaza Genocide}}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{primaryColor}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{0.5pt}

% --- Section Styling ---
\titleformat{\section}
{\color{primaryColor}\normalfont\Large\bfseries}
{\thesection}{1em}{}

\titleformat{\subsection}
{\color{secondaryColor}\normalfont\large\bfseries}
{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\color{secondaryColor}\normalfont\normalsize\bfseries}
{\thesubsubsection}{1em}{}

\begin{document}

% ==============================================================================
% COVER PAGE
% ==============================================================================
\begin{titlepage}

    \centering
    {\large \textbf{Ministry of Higher Education and Scientific Research}} \\
    \vspace{0.2cm}
    {\large \textbf{University of Manouba}} \\
    \vspace{0.2cm}
    {\large \textbf{Higher Institute of Multimedia Arts (ISAMM)}}
    
    \vspace{2cm}
    
    % ISAMM Logo Placeholer
    % Ensure 'isamm.png' exists in your folder
    \includegraphics[width=0.35\textwidth]{isamm.png} 
    
    \vspace{2cm}
    
    % Title Section
    {\color{primaryColor} \hrule height 2pt}
    \vspace{0.6cm}
    {\uppercase{\huge \bfseries \color{secondaryColor} Big Data Analysis}} \\
    \vspace{0.4cm}
    {\Large \bfseries YouTube Content Related to the Gaza Genocide}
    \vspace{0.6cm}
    {\color{primaryColor} \hrule height 2pt}
    
    \vspace{2.5cm}
    
    % Author and Supervisor
    \begin{minipage}{0.45\textwidth}
        \begin{flushleft}
            \textbf{\color{primaryColor} Prepared by:} \\ 
            \vspace{0.2cm}
            \large Mohamed Ayacha \\
            \small 3IM1
        \end{flushleft}
    \end{minipage}

    \vfill
    
    % Footer Year
    \textbf{Academic Year:} 2025/2026
    
\end{titlepage}

% ==============================================================================
% TABLE OF CONTENTS & LISTS
% ==============================================================================
{
  \hypersetup{linkcolor=black} % Keep TOC black for readability
  \setcounter{tocdepth}{3}     % Ensure subsubsections appear in TOC
  
  \tableofcontents
  \newpage
  
  % List of Figures
  \addcontentsline{toc}{section}{\listfigurename}
  \listoffigures
  \newpage
  
  % List of Code Snippets (Listings)
  \addcontentsline{toc}{section}{List of Code Snippets}
  \lstlistoflistings
  \newpage

  % List of Tables
  \addcontentsline{toc}{section}{\listtablename}
  \listoftables
  \newpage
}

% ==============================================================================
% ABSTRACT
% ==============================================================================
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

This report details the design and implementation of a comprehensive Big Data analytical pipeline tailored to process and analyze YouTube data concerning the ongoing genocide in Gaza. Leveraging the \textbf{YouTube Data API v3}, the project systematically aggregates video metadata and user comments from October 2023 to October 2025. The infrastructure employs a distributed architecture featuring \textbf{Hadoop HDFS} for resilient data storage and \textbf{Apache Spark} concepts for processing. The analysis, executed via Python, uncovers critical insights into global public engagement, the dominance of major media outlets, and the temporal evolution of digital discourse. 

\textbf{New Features:} This version includes an interactive \textbf{Graphical User Interface (GUI)} built with Tkinter, enabling users to configure data collection parameters (date ranges, search queries, result limits) dynamically without modifying code. The interface features a comprehensive image gallery with \textbf{arrow key navigation} for browsing generated charts. Advanced \textbf{keyword stemming} using Porter Stemmer algorithms normalizes text data, reducing keywords to their root forms for more accurate frequency analysis. All user inputs from the GUI are seamlessly integrated into the data processing pipeline for end-to-end automation.

This work demonstrates the efficacy of Big Data technologies in interpreting complex, high-volume social media trends, complemented by user-friendly tools for interactive analysis.

\newpage

% ==============================================================================
% INTRODUCTION
% ==============================================================================
\section{Introduction \& Problem Statement}

\begin{figure}[H]
    \centering
    % Ensure 'logo.png' exists
    \includegraphics[width=0.25\textwidth]{logo.png}
    \caption{Project Logo}
\end{figure}

The digital age has fundamentally altered the landscape of geopolitical discourse. Social media platforms, particularly YouTube, serve not only as repositories of user-generated content but as primary sources of news and information for millions globally. The genocide in Gaza has generated an unprecedented volume of digital interaction, providing a unique opportunity to apply data science techniques to understand global sentiment.

\subsection{Problem Identification}
Analyzing this vast ocean of data presents significant challenges:
\begin{itemize}
    \item \textbf{Volume \& Velocity:} The rate of video uploads and comment generation exceeds the capacity of traditional manual analysis or single-threaded processing tools.
    \item \textbf{Variety:} The data is highly heterogeneous, comprising unstructured text (titles, descriptions, comments), semi-structured metadata (JSON), and numerical engagement metrics.
    \item \textbf{Veracity:} The politicized nature of the topic requires rigorous data collection methods to ensure a representative sample.
\end{itemize}

\subsection{Project Goals}
The primary objective is to build a scalable pipeline that can:
\begin{enumerate}
    \item Ingest data reliably from external APIs with user-configurable parameters.
    \item Store large datasets in a fault-tolerant manner.
    \item Process and transform raw data into analytical insights with advanced text normalization.
    \item Visualize trends to make complex data accessible through interactive galleries.
    \item Provide an intuitive user interface for non-technical end-users to configure and execute the pipeline.
\end{enumerate}

% ==============================================================================
% SOLUTION
% ==============================================================================
\section{Proposed Solution: The Big Data Pipeline}
To address these challenges, we designed a pipeline grounded in the \textbf{Lambda Architecture} principles, balancing batch processing with scalable storage. The solution automates the end-to-end flow from data acquisition to insight generation.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{"Global Big Data Architecture"}
    \caption{Overview of the Big Data Architecture from Data Source to Visualization.}
\end{figure}

The architecture shown above details the complete data journey. Data flows from the YouTube API through our Python Collector, settles in HDFS, moves to Spark for heavy processing, and finally to Pandas for granular analysis and visualization.

\subsection{Analytical Approach}
We employ a mixed-methods approach:
\begin{itemize}
    \item \textbf{Descriptive Analytics:} To understand "what happened" (e.g., total views, upload frequency).
    \item \textbf{Diagnostic Analytics:} To understand "why it happened" (e.g., correlating spikes in views with real-world events).
    \item \textbf{Text Mining:} To extract key themes and dominant narratives from video titles.
\end{itemize}

% ==============================================================================
% SYSTEM ARCHITECTURE
% ==============================================================================
\section{System Architecture \& Technology Stack}

The architecture is designed for modularity and scalability. Each component plays a specific role in the data lifecycle.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth,height=0.4\textheight,keepaspectratio]{"Detailed Data Flow Diagram"}
    \caption{Detailed Data Flow Diagram illustrating the interaction between HDFS, Spark, and Python scripts.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth,height=0.4\textheight,keepaspectratio]{"Distributed Processing with Spark"}
    \caption{Distributed Processing architecture, highlighting the parallel execution capabilities of the Spark engine.}
\end{figure}

As illustrated, the system separates concerns between storage (HDFS) and compute (Spark/Python). This \textbf{decoupling} allows for independent scaling of resources.

\subsection{Technology Stack}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includesvg[width=0.7\textwidth]{Hadoop_logo}
        \caption{Hadoop HDFS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includesvg[width=0.7\textwidth]{Apache_Spark_logo}
        \caption{Apache Spark}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includesvg[width=0.5\textwidth]{Python-logo-notext}
        \caption{Python}
    \end{subfigure}
    \caption{Core Technologies Used}
\end{figure}

\begin{itemize}
    \item \textbf{YouTube Data API v3:} The extraction layer. It allows granular search capabilities, enabling us to filter content by date, relevance, and region.
    \item \textbf{Hadoop HDFS:} The storage layer. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. It ensures data is replicated across nodes to prevent loss.
    \item \textbf{Apache Spark:} The processing layer. Spark's in-memory computation capabilities allow for rapid iteration over large datasets, significantly faster than traditional MapReduce.
    \item \textbf{Python (Pandas, Matplotlib, Seaborn):} The analysis and presentation layer. Python's rich ecosystem of data science libraries makes it ideal for statistical analysis and generating publication-quality figures.
\end{itemize}

% ==============================================================================
% IMPLEMENTATION
% ==============================================================================
\section{Implementation Details}

The project execution followed a strict software development lifecycle suitable for data engineering projects.

\subsection{Phase 1: Environment Configuration}
Before coding, a robust Big Data environment was established. We utilized a pre-configured Virtual Machine mimicking a single-node cluster.
\begin{itemize}
    \item \textbf{HDFS Setup:} Configured hdfs-site.xml and core-site.xml to define replication factors and NameNode locations.
    \item \textbf{Service Verification:} Verified NameNode, DataNode, and ResourceManager daemons were active using the Java Process Status (jps) tool.
\end{itemize}

\subsubsection*{Environment Setup Challenges}
Due to persistent technical challenges encountered while attempting to configure Docker on my local machine, I opted to utilize the pre-configured Virtual Machine provided by our lab instructor. This decision allowed me to proceed with the project by following the configurations outlined in Labs 1 and 2.

\subsection{Phase 2: Data Ingestion (The Collector)}
We developed a custom Python module, src/data\_collector.py, to interface with the Google Cloud Platform. The script implements:
\begin{itemize}
    \item \textbf{Rate Limiting:} To respect API quotas.
    \item \textbf{Pagination:} Handling nextPageToken to retrieve deep search results beyond the first page.
    \item \textbf{Error Handling:} Robust try-except blocks to manage network timeouts or API errors.
\end{itemize}

The search queries were carefully selected to cover various aspects of the conflict:
\textit{"Gaza war", "Israel Palestine conflict", "Gaza humanitarian crisis", "Palestine news", "Israel Hamas war"}.

\begin{lstlisting}[caption={Core Search Logic in data\_collector.py}]
def search_videos(self, query, max_results=50, published_after=None, published_before=None):
    """
    Executes a search query against the YouTube API.
    Supports filtering by date range for temporal analysis.
    """
    url = f"{self.base_url}/search"
    params = {
        'part': 'snippet',
        'q': query,
        'type': 'video',
        'maxResults': min(max_results, 50),
        'key': self.api_key,
        'order': 'date', # Sort by date to get a timeline
        'publishedAfter': published_after,
        'publishedBefore': published_before
    }
    # ... Request handling ...
\end{lstlisting}

\subsection{Phase 3: Distributed Storage (HDFS Integration)}
Once collected, data is migrated from the local file system to the Hadoop Distributed File System. This step mimics a production ETL (Extract, Transform, Load) process.

\begin{lstlisting}[language=bash, caption={HDFS Shell Commands}]
# 1. Create a dedicated directory structure
hdfs dfs -mkdir -p /user/project/youtube_data

# 2. Upload the JSON datasets
hdfs dfs -put data/youtube_videos.json /user/project/youtube_data/
hdfs dfs -put data/youtube_comments.json /user/project/youtube_data/

# 3. Verify data integrity
hdfs dfs -ls /user/project/youtube_data/
\end{lstlisting}

\subsection{Phase 4: Data Processing \& Analysis}
The src/data\_analyzer.py script serves as the engine for extracting insights. It performs several key transformations:
\begin{enumerate}
    \item \textbf{Data Cleaning:} Converting string dates (ISO 8601) into Python datetime objects for temporal sorting.
    \item \textbf{Type Casting:} Ensuring numerical fields like viewCount and likeCount are treated as integers/floats, handling any missing values (NaN) by filling them with zeros.
    \item \textbf{Normalization:} Processing text fields (lowercasing, removing stopwords) to prepare for keyword frequency analysis.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{"Logical Processing Steps"}
    \caption{The logical steps taken during the processing phase, from raw JSON ingestion to cleaned DataFrames.}
\end{figure}

We specifically chose a hybrid approach, using Spark concepts for the heavy lifting design and Python for implementation flexibility.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{"Hybrid Analytics Approach"}
    \caption{Illustration of the Hybrid Analytics Model combining Spark's conceptual scalability with Pandas' analytical agility.}
\end{figure}

\begin{lstlisting}[caption={Text normalization and Word Count}]
# Tokenizing titles to find most frequent words
all_words = ' '.join(df_videos['title'].str.lower()).split()
stop_words = {'the', 'and', 'for', 'with', 'to', 'in', 'of', 'a', 'is'}
word_counts = Counter([word for word in all_words if len(word) > 3 and word not in stop_words])
\end{lstlisting}

\subsection{Phase 5: Keyword Normalization \& Stemming}
A critical enhancement is the implementation of advanced keyword normalization through the Porter Stemmer algorithm in src/utils.py. This ensures that keyword frequency analysis captures the true semantic content of video titles.

\begin{lstlisting}[caption={Stemming Implementation in utils.py}]
def stem_word(word):
    """Apply Porter stemming to reduce words to root form."""
    word = word.lower()
    if len(word) <= 3:
        return word
    # Apply stemming rules...
    return word

def normalize_text(text, use_stemming=True):
    """Normalize text by removing stopwords and applying stemming."""
    stop_words = get_stop_words()
    words = re.findall(r'\b\w+\b', text.lower())
    normalized = []
    for word in words:
        if word not in stop_words:
            if use_stemming:
                normalized.append(stem_word(word))
            else:
                normalized.append(word)
    return normalized
\end{lstlisting}

% ==============================================================================
% GUI SECTION (Optimized for One Image)
% ==============================================================================
\subsection{Phase 6: Graphical User Interface (GUI)}
A major addition to the project is the comprehensive Graphical User Interface (src/gui.py) that democratizes access to the data pipeline. The application combines configuration, execution, and visualization into a single unified window.

\begin{figure}[H]
    \centering
    % !!! REPLACE THIS WITH YOUR SINGLE GUI IMAGE !!!
    \includegraphics[width=0.95\textwidth]{gui_main_overview.jpg} 
    \caption{The Graphical User Interface, showcasing the Data Configuration Panel and the Interactive Results Gallery.}
    \label{fig:gui_main}
\end{figure}

\subsubsection{Key GUI Features}
As depicted in Figure \ref{fig:gui_main}, the interface offers three primary functionalities:

\begin{enumerate}
    \item \textbf{Configuration Panel:}
    Users can input their API Key, specify custom search queries (one per line), and select date ranges using interactive calendar widgets. Spinbox controls allow for setting precise limits on the number of videos and comments to collect.
    
    \item \textbf{Real-Time Progress Tracking:}
    Upon execution, the system provides immediate feedback via a progress bar and status messages (e.g., "Fetching videos for query: Gaza war"). The application employs threading to ensure the interface remains responsive during heavy data fetching operations.
    
    \item \textbf{Interactive Image Gallery:}
    The built-in gallery allows users to browse generated analysis charts immediately. It supports *Arrow Key Navigation* (Left/Right) for rapid browsing, features dynamic image loading from the output directory, and automatically scales charts to fit the window.
\end{enumerate}

\begin{lstlisting}[caption={Pipeline Execution Architecture}]
class PipelineExecutor:
    """Executes the complete data collection, analysis, and visualization pipeline."""
    
    def run(self):
        """Execute pipeline stages sequentially."""
        self.run_collector()  # Collect data with user parameters
        self.run_analyzer()   # Perform analysis with stemming
        self.run_visualizer() # Generate charts
        self.complete_callback()  # Update UI
\end{lstlisting}

% ==============================================================================
% RESULTS
% ==============================================================================
\section{Analysis Results}

The analysis of the dataset collected between Oct 2023 and Oct 2025 yielded significant findings regarding the digital coverage of the conflict.

\subsection{Media Dominance}
The analysis of the "Top Channels" clearly indicates that legacy media and established international news outlets dominate the conversation on YouTube. Unlike other social platforms where individual content creators may lead, YouTube searches for this conflict act primarily as a news aggregator.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{top_channels.png} 
    \caption{Top 10 Channels by Video Count. Note the prevalence of major news networks.}
\end{figure}

\subsection{Temporal Evolution}
The timeline of video uploads is not linear. It exhibits sharp peaks that correlate strongly with real-world escalation events. This suggests that content creation is highly reactive.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{timeline.png} 
    \caption{Video upload frequency over time. Peaks correspond to major news events.}
\end{figure}

\subsection{Keyword Analysis with Stemming}
The textual analysis of video titles is enhanced through Porter stemming. Keywords like "War", "Attack", and "Crisis" are far more prevalent than "Peace" or "Resolution", indicating a media focus on the kinetic aspects of the conflict.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{top_words.png} 
    \caption{Most frequent stemmed keywords. The vocabulary is dominated by conflict-related terminology.}
\end{figure}

\subsection{Engagement Metrics}
The query performance chart highlights a disparity in public interest. Search terms related to active conflict ("War", "Conflict") garner significantly more views and likes than those related to humanitarian aspects ("Crisis").

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{query_performance.png} 
    \caption{Engagement (Views/Likes) distributed by search query.}
\end{figure}

\newpage

% ==============================================================================
% CONCLUSION
% ==============================================================================
\section{Conclusion \& Future Work}

\subsection{Conclusion}
This project has successfully demonstrated the implementation of a Big Data pipeline for social media analysis with user-centric design principles. By integrating HDFS for storage, Python/Spark for processing, and a comprehensive GUI for ease of use, we have created a scalable, accessible framework capable of handling the velocity and variety of YouTube data.

Key takeaways include:
\begin{enumerate}
    \item \textbf{Scalability:} The architecture is decoupled, allowing storage and compute to scale independently.
    \item \textbf{Accessibility:} The Tkinter GUI democratizes access to complex data pipelines.
    \item \textbf{Text Intelligence:} Advanced keyword stemming normalizes terminology, reducing noise.
    \item \textbf{Insight:} The analysis confirms that digital discourse on the Gaza genocide is news-driven and reactive.
\end{enumerate}

\subsection{Future Enhancements}
To further evolve this project, we propose:
\begin{itemize}
    \item \textbf{Sentiment Analysis:} Integrating NLP models (e.g., VADER or BERT) to classify sentiment.
    \item \textbf{Real-Time Streaming:} Implementing Apache Kafka for real-time ingestion.
    \item \textbf{Geospatial Analysis:} Mapping the origin of comments by region.
\end{itemize}

\section{Appendix: Technical Specifications}

\subsection{System Requirements}
\begin{itemize}
    \item Python 3.8 or higher
    \item Hadoop 3.x (optional, for HDFS integration)
    \item Apache Spark 3.x (optional, for distributed processing)
    \item Required Python packages: \texttt{pyspark}, \texttt{pandas}, \texttt{matplotlib}, \texttt{seaborn}, \texttt{nltk}, \texttt{tkcalendar}.
\end{itemize}

\subsection{GUI Features Summary}
\begin{table}[H]
    \centering
    \begin{tabular}{@{}llp{8cm}@{}}
        \toprule
        \textbf{Feature} & \textbf{View} & \textbf{Description} \\
        \midrule
        Date Selection & Config & Interactive calendar widgets for start/end dates. \\
        Query Input & Config & Multi-line text area for custom search queries. \\
        Result Limits & Config & Spinbox controls for max videos and comments. \\
        Progress Tracking & Progress & Real-time status updates and progress bar. \\
        Arrow Navigation & Gallery & Left/Right arrow keys to browse images. \\
        Responsive UI & All & Thread-safe execution prevents UI freezing. \\
        \bottomrule
    \end{tabular}
    \caption{GUI Feature Matrix}
\end{table}

\end{document}